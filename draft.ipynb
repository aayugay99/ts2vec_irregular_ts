{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from ptls.preprocessing import PandasDataPreprocessor\n",
    "from ptls.data_load.utils import collate_feature_dict\n",
    "from ptls.data_load.iterable_processing import SeqLenFilter\n",
    "from ptls.data_load.datasets.memory_dataset import MemoryMapDataset\n",
    "\n",
    "from ptls.nn import TrxEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/preprocessed_new/churn.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>mcc_code</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>amount</th>\n",
       "      <th>global_target</th>\n",
       "      <th>holiday_target</th>\n",
       "      <th>weekend_target</th>\n",
       "      <th>churn_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2017-10-21 00:00:00</td>\n",
       "      <td>5023.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-10-12 12:24:07</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2017-12-05 00:00:00</td>\n",
       "      <td>767.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-10-21 00:00:00</td>\n",
       "      <td>2031.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2017-10-24 13:14:24</td>\n",
       "      <td>36562.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490508</th>\n",
       "      <td>10215</td>\n",
       "      <td>37</td>\n",
       "      <td>2016-12-17 00:00:00</td>\n",
       "      <td>2110.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490509</th>\n",
       "      <td>10215</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-12-16 00:00:00</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490510</th>\n",
       "      <td>10215</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-12-06 00:00:00</td>\n",
       "      <td>182.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490511</th>\n",
       "      <td>10215</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-12-06 13:39:49</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490512</th>\n",
       "      <td>10215</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-12-06 13:42:19</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>490513 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  mcc_code           timestamp   amount  global_target  \\\n",
       "0             0        19 2017-10-21 00:00:00   5023.0              0   \n",
       "1             0         2 2017-10-12 12:24:07  20000.0              0   \n",
       "2             0        10 2017-12-05 00:00:00    767.0              0   \n",
       "3             0         1 2017-10-21 00:00:00   2031.0              0   \n",
       "4             0         9 2017-10-24 13:14:24  36562.0              0   \n",
       "...         ...       ...                 ...      ...            ...   \n",
       "490508    10215        37 2016-12-17 00:00:00   2110.9              0   \n",
       "490509    10215         1 2016-12-16 00:00:00     31.0              0   \n",
       "490510    10215         1 2016-12-06 00:00:00    182.0              0   \n",
       "490511    10215         2 2016-12-06 13:39:49   5000.0              0   \n",
       "490512    10215         2 2016-12-06 13:42:19  30000.0              0   \n",
       "\n",
       "        holiday_target  weekend_target  churn_target  \n",
       "0                    0               1             0  \n",
       "1                    0               0             0  \n",
       "2                    0               0             0  \n",
       "3                    0               1             0  \n",
       "4                    0               0             0  \n",
       "...                ...             ...           ...  \n",
       "490508               0               1             0  \n",
       "490509               0               0             0  \n",
       "490510               0               0             0  \n",
       "490511               0               0             0  \n",
       "490512               0               0             0  \n",
       "\n",
       "[490513 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = PandasDataPreprocessor(\n",
    "    col_id=\"user_id\",\n",
    "    col_event_time=\"timestamp\",\n",
    "    event_time_transformation=\"dt_to_timestamp\",\n",
    "    cols_category=[\"mcc_code\"],\n",
    "    cols_numerical=[\"amount\"],\n",
    "    cols_first_item=\"global_target\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preproc.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MemoryMapDataset(data, [SeqLenFilter(15)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataset, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trx_encoder = TrxEncoder(\n",
    "    embeddings={\n",
    "        \"mcc_code\": {\"in\": 345, \"out\": 24}\n",
    "    },\n",
    "    numeric_values={\n",
    "        \"amount\": \"identity\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from models.dilated_conv import DilatedConvEncoder\n",
    "from ptls.data_load import PaddedBatch\n",
    "\n",
    "def generate_continuous_mask(B, T, n=5, l=0.1):\n",
    "    res = torch.full((B, T), True, dtype=torch.bool)\n",
    "    if isinstance(n, float):\n",
    "        n = int(n * T)\n",
    "    n = max(min(n, T // 2), 1)\n",
    "    \n",
    "    if isinstance(l, float):\n",
    "        l = int(l * T)\n",
    "    l = max(l, 1)\n",
    "    \n",
    "    for i in range(B):\n",
    "        for _ in range(n):\n",
    "            t = np.random.randint(T-l+1)\n",
    "            res[i, t:t+l] = False\n",
    "    return res\n",
    "\n",
    "def generate_binomial_mask(B, T, p=0.5):\n",
    "    return torch.from_numpy(np.random.binomial(1, p, size=(B, T))).to(torch.bool)\n",
    "\n",
    "class TSEncoder(nn.Module):\n",
    "    def __init__(self, trx_encoder, output_dims, depth=10, mask_mode='binomial'):\n",
    "        super().__init__()\n",
    "        self.trx_encoder = trx_encoder\n",
    "        self.output_dims = output_dims\n",
    "        self.hidden_dims = trx_encoder.output_size\n",
    "        self.mask_mode = mask_mode\n",
    "        # self.input_fc = nn.Linear(input_dims, hidden_dims)\n",
    "        self.feature_extractor = DilatedConvEncoder(\n",
    "            self.hidden_dims,\n",
    "            [self.hidden_dims] * depth + [output_dims],\n",
    "            kernel_size=3\n",
    "        )\n",
    "        self.repr_dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "    def forward(self, x, mask=None):  # x: B x T x input_dims\n",
    "        if mask is None:\n",
    "            if self.training:\n",
    "                mask = self.mask_mode\n",
    "            else:\n",
    "                mask = 'all_true'\n",
    "        \n",
    "        if mask == 'binomial':\n",
    "            mask = generate_binomial_mask(x.size(0), x.size(1)).to(x.device)\n",
    "        elif mask == 'continuous':\n",
    "            mask = generate_continuous_mask(x.size(0), x.size(1)).to(x.device)\n",
    "        elif mask == 'all_true':\n",
    "            mask = x.new_full((x.size(0), x.size(1)), True, dtype=torch.bool)\n",
    "        elif mask == 'all_false':\n",
    "            mask = x.new_full((x.size(0), x.size(1)), False, dtype=torch.bool)\n",
    "        elif mask == 'mask_last':\n",
    "            mask = x.new_full((x.size(0), x.size(1)), True, dtype=torch.bool)\n",
    "            mask[:, -1] = False\n",
    "        \n",
    "        # mask &= nan_mask\n",
    "        x[~mask] = 0\n",
    "        \n",
    "        # conv encoder\n",
    "        x = x.transpose(1, 2)  # B x Ch x T\n",
    "        x = self.repr_dropout(self.feature_extractor(x))  # B x Co x T\n",
    "        x = x.transpose(1, 2)  # B x T x Co\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def encode(self, x: PaddedBatch):\n",
    "        return self.trx_encoder(x).payload\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "# from models import TSEncoder\n",
    "from models.losses import hierarchical_contrastive_loss\n",
    "from utils import take_per_row, split_with_nan, centerize_vary_length_series, torch_pad_nan\n",
    "import math\n",
    "\n",
    "class TS2Vec:\n",
    "    '''The TS2Vec model'''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        trx_encoder,\n",
    "        output_dims=320,\n",
    "        depth=10,\n",
    "        device='cuda',\n",
    "        lr=0.001,\n",
    "        batch_size=16,\n",
    "        max_train_length=None,\n",
    "        temporal_unit=0,\n",
    "        after_iter_callback=None,\n",
    "        after_epoch_callback=None\n",
    "    ):\n",
    "        ''' Initialize a TS2Vec model.\n",
    "        \n",
    "        Args:\n",
    "            input_dims (int): The input dimension. For a univariate time series, this should be set to 1.\n",
    "            output_dims (int): The representation dimension.\n",
    "            hidden_dims (int): The hidden dimension of the encoder.\n",
    "            depth (int): The number of hidden residual blocks in the encoder.\n",
    "            device (int): The gpu used for training and inference.\n",
    "            lr (int): The learning rate.\n",
    "            batch_size (int): The batch size.\n",
    "            max_train_length (Union[int, NoneType]): The maximum allowed sequence length for training. For sequence with a length greater than <max_train_length>, it would be cropped into some sequences, each of which has a length less than <max_train_length>.\n",
    "            temporal_unit (int): The minimum unit to perform temporal contrast. When training on a very long sequence, this param helps to reduce the cost of time and memory.\n",
    "            after_iter_callback (Union[Callable, NoneType]): A callback function that would be called after each iteration.\n",
    "            after_epoch_callback (Union[Callable, NoneType]): A callback function that would be called after each epoch.\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.max_train_length = max_train_length\n",
    "        self.temporal_unit = temporal_unit\n",
    "        \n",
    "        self._net = TSEncoder(trx_encoder=trx_encoder, output_dims=output_dims, depth=depth).to(self.device)\n",
    "        self.net = torch.optim.swa_utils.AveragedModel(self._net)\n",
    "        self.net.update_parameters(self._net)\n",
    "        \n",
    "        self.after_iter_callback = after_iter_callback\n",
    "        self.after_epoch_callback = after_epoch_callback\n",
    "        \n",
    "        self.n_epochs = 0\n",
    "        self.n_iters = 0\n",
    "    \n",
    "    def fit(self, train_dataset, n_epochs=None, n_iters=None, verbose=False):\n",
    "        ''' Training the TS2Vec model.\n",
    "        \n",
    "        Args:\n",
    "            train_data (numpy.ndarray): The training data. It should have a shape of (n_instance, n_timestamps, n_features). All missing data should be set to NaN.\n",
    "            n_epochs (Union[int, NoneType]): The number of epochs. When this reaches, the training stops.\n",
    "            n_iters (Union[int, NoneType]): The number of iterations. When this reaches, the training stops. If both n_epochs and n_iters are not specified, a default setting would be used that sets n_iters to 200 for a dataset with size <= 100000, 600 otherwise.\n",
    "            verbose (bool): Whether to print the training loss after each epoch.\n",
    "            \n",
    "        Returns:\n",
    "            loss_log: a list containing the training losses on each epoch.\n",
    "        '''\n",
    "        # assert train_data.ndim == 3\n",
    "        \n",
    "        # if n_iters is None and n_epochs is None:\n",
    "        #     n_iters = 200 if train_data.size <= 100000 else 600  # default param for n_iters\n",
    "        \n",
    "        # if self.max_train_length is not None:\n",
    "        #     sections = train_data.shape[1] // self.max_train_length\n",
    "        #     if sections >= 2:\n",
    "        #         train_data = np.concatenate(split_with_nan(train_data, sections, axis=1), axis=0)\n",
    "\n",
    "        # temporal_missing = np.isnan(train_data).all(axis=-1).any(axis=0)\n",
    "        # if temporal_missing[0] or temporal_missing[-1]:\n",
    "        #     train_data = centerize_vary_length_series(train_data)\n",
    "                \n",
    "        # train_data = train_data[~np.isnan(train_data).all(axis=2).all(axis=1)]\n",
    "        \n",
    "        # train_dataset = TensorDataset(torch.from_numpy(train_data).to(torch.float))\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=min(self.batch_size, len(train_dataset)), \n",
    "            shuffle=True, \n",
    "            drop_last=True,\n",
    "            collate_fn=collate_feature_dict\n",
    "        )\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(self._net.parameters(), lr=self.lr)\n",
    "        \n",
    "        loss_log = []\n",
    "        \n",
    "        while True:\n",
    "            if n_epochs is not None and self.n_epochs >= n_epochs:\n",
    "                break\n",
    "            \n",
    "            cum_loss = 0\n",
    "            n_epoch_iters = 0\n",
    "            \n",
    "            interrupted = False\n",
    "            for batch in train_loader:\n",
    "                if n_iters is not None and self.n_iters >= n_iters:\n",
    "                    interrupted = True\n",
    "                    break\n",
    "                \n",
    "                x = self._net.encode(batch.to(self.device))\n",
    "                if self.max_train_length is not None and x.size(1) > self.max_train_length:\n",
    "                    window_offset = np.random.randint(x.size(1) - self.max_train_length + 1)\n",
    "                    x = x[:, window_offset : window_offset + self.max_train_length]\n",
    "                # x = x.to(self.device)\n",
    "                \n",
    "                ts_l = x.size(1)\n",
    "                crop_l = np.random.randint(low=2 ** (self.temporal_unit + 1), high=ts_l+1)\n",
    "                crop_left = np.random.randint(ts_l - crop_l + 1)\n",
    "                crop_right = crop_left + crop_l\n",
    "                crop_eleft = np.random.randint(crop_left + 1)\n",
    "                crop_eright = np.random.randint(low=crop_right, high=ts_l + 1)\n",
    "                crop_offset = np.random.randint(low=-crop_eleft, high=ts_l - crop_eright + 1, size=x.size(0))\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                out1 = self._net(take_per_row(x, crop_offset + crop_eleft, crop_right - crop_eleft))\n",
    "                out1 = out1[:, -crop_l:]\n",
    "                \n",
    "                out2 = self._net(take_per_row(x, crop_offset + crop_left, crop_eright - crop_left))\n",
    "                out2 = out2[:, :crop_l]\n",
    "                \n",
    "                loss = hierarchical_contrastive_loss(\n",
    "                    out1,\n",
    "                    out2,\n",
    "                    temporal_unit=self.temporal_unit\n",
    "                )\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                self.net.update_parameters(self._net)\n",
    "                    \n",
    "                cum_loss += loss.item()\n",
    "                n_epoch_iters += 1\n",
    "                \n",
    "                self.n_iters += 1\n",
    "                \n",
    "                if self.after_iter_callback is not None:\n",
    "                    self.after_iter_callback(self, loss.item())\n",
    "            \n",
    "            if interrupted:\n",
    "                break\n",
    "            \n",
    "            cum_loss /= n_epoch_iters\n",
    "            loss_log.append(cum_loss)\n",
    "            if verbose:\n",
    "                print(f\"Epoch #{self.n_epochs}: loss={cum_loss}\")\n",
    "            self.n_epochs += 1\n",
    "            \n",
    "            if self.after_epoch_callback is not None:\n",
    "                self.after_epoch_callback(self, cum_loss)\n",
    "            \n",
    "        return loss_log\n",
    "    \n",
    "    def _eval_with_pooling(self, x, mask=None, slicing=None, encoding_window=None):\n",
    "        out = self.net(x.to(self.device, non_blocking=True), mask)\n",
    "        if encoding_window == 'full_series':\n",
    "            if slicing is not None:\n",
    "                out = out[:, slicing]\n",
    "            out = F.max_pool1d(\n",
    "                out.transpose(1, 2),\n",
    "                kernel_size = out.size(1),\n",
    "            ).transpose(1, 2)\n",
    "            \n",
    "        elif isinstance(encoding_window, int):\n",
    "            out = F.max_pool1d(\n",
    "                out.transpose(1, 2),\n",
    "                kernel_size = encoding_window,\n",
    "                stride = 1,\n",
    "                padding = encoding_window // 2\n",
    "            ).transpose(1, 2)\n",
    "            if encoding_window % 2 == 0:\n",
    "                out = out[:, :-1]\n",
    "            if slicing is not None:\n",
    "                out = out[:, slicing]\n",
    "            \n",
    "        elif encoding_window == 'multiscale':\n",
    "            p = 0\n",
    "            reprs = []\n",
    "            while (1 << p) + 1 < out.size(1):\n",
    "                t_out = F.max_pool1d(\n",
    "                    out.transpose(1, 2),\n",
    "                    kernel_size = (1 << (p + 1)) + 1,\n",
    "                    stride = 1,\n",
    "                    padding = 1 << p\n",
    "                ).transpose(1, 2)\n",
    "                if slicing is not None:\n",
    "                    t_out = t_out[:, slicing]\n",
    "                reprs.append(t_out)\n",
    "                p += 1\n",
    "            out = torch.cat(reprs, dim=-1)\n",
    "            \n",
    "        else:\n",
    "            if slicing is not None:\n",
    "                out = out[:, slicing]\n",
    "            \n",
    "        return out.cpu()\n",
    "    \n",
    "    def encode(self, data, mask=None, encoding_window=None, causal=False, sliding_length=None, sliding_padding=0, batch_size=None):\n",
    "        ''' Compute representations using the model.\n",
    "        \n",
    "        Args:\n",
    "            data (numpy.ndarray): This should have a shape of (n_instance, n_timestamps, n_features). All missing data should be set to NaN.\n",
    "            mask (str): The mask used by encoder can be specified with this parameter. This can be set to 'binomial', 'continuous', 'all_true', 'all_false' or 'mask_last'.\n",
    "            encoding_window (Union[str, int]): When this param is specified, the computed representation would the max pooling over this window. This can be set to 'full_series', 'multiscale' or an integer specifying the pooling kernel size.\n",
    "            causal (bool): When this param is set to True, the future informations would not be encoded into representation of each timestamp.\n",
    "            sliding_length (Union[int, NoneType]): The length of sliding window. When this param is specified, a sliding inference would be applied on the time series.\n",
    "            sliding_padding (int): This param specifies the contextual data length used for inference every sliding windows.\n",
    "            batch_size (Union[int, NoneType]): The batch size used for inference. If not specified, this would be the same batch size as training.\n",
    "            \n",
    "        Returns:\n",
    "            repr: The representations for data.\n",
    "        '''\n",
    "        assert self.net is not None, 'please train or load a net first'\n",
    "        assert data.ndim == 3\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        n_samples, ts_l, _ = data.shape\n",
    "\n",
    "        org_training = self.net.training\n",
    "        self.net.eval()\n",
    "        \n",
    "        dataset = TensorDataset(torch.from_numpy(data).to(torch.float))\n",
    "        loader = DataLoader(dataset, batch_size=batch_size)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = []\n",
    "            for batch in loader:\n",
    "                x = batch[0]\n",
    "                if sliding_length is not None:\n",
    "                    reprs = []\n",
    "                    if n_samples < batch_size:\n",
    "                        calc_buffer = []\n",
    "                        calc_buffer_l = 0\n",
    "                    for i in range(0, ts_l, sliding_length):\n",
    "                        l = i - sliding_padding\n",
    "                        r = i + sliding_length + (sliding_padding if not causal else 0)\n",
    "                        x_sliding = torch_pad_nan(\n",
    "                            x[:, max(l, 0) : min(r, ts_l)],\n",
    "                            left=-l if l<0 else 0,\n",
    "                            right=r-ts_l if r>ts_l else 0,\n",
    "                            dim=1\n",
    "                        )\n",
    "                        if n_samples < batch_size:\n",
    "                            if calc_buffer_l + n_samples > batch_size:\n",
    "                                out = self._eval_with_pooling(\n",
    "                                    torch.cat(calc_buffer, dim=0),\n",
    "                                    mask,\n",
    "                                    slicing=slice(sliding_padding, sliding_padding+sliding_length),\n",
    "                                    encoding_window=encoding_window\n",
    "                                )\n",
    "                                reprs += torch.split(out, n_samples)\n",
    "                                calc_buffer = []\n",
    "                                calc_buffer_l = 0\n",
    "                            calc_buffer.append(x_sliding)\n",
    "                            calc_buffer_l += n_samples\n",
    "                        else:\n",
    "                            out = self._eval_with_pooling(\n",
    "                                x_sliding,\n",
    "                                mask,\n",
    "                                slicing=slice(sliding_padding, sliding_padding+sliding_length),\n",
    "                                encoding_window=encoding_window\n",
    "                            )\n",
    "                            reprs.append(out)\n",
    "\n",
    "                    if n_samples < batch_size:\n",
    "                        if calc_buffer_l > 0:\n",
    "                            out = self._eval_with_pooling(\n",
    "                                torch.cat(calc_buffer, dim=0),\n",
    "                                mask,\n",
    "                                slicing=slice(sliding_padding, sliding_padding+sliding_length),\n",
    "                                encoding_window=encoding_window\n",
    "                            )\n",
    "                            reprs += torch.split(out, n_samples)\n",
    "                            calc_buffer = []\n",
    "                            calc_buffer_l = 0\n",
    "                    \n",
    "                    out = torch.cat(reprs, dim=1)\n",
    "                    if encoding_window == 'full_series':\n",
    "                        out = F.max_pool1d(\n",
    "                            out.transpose(1, 2).contiguous(),\n",
    "                            kernel_size = out.size(1),\n",
    "                        ).squeeze(1)\n",
    "                else:\n",
    "                    out = self._eval_with_pooling(x, mask, encoding_window=encoding_window)\n",
    "                    if encoding_window == 'full_series':\n",
    "                        out = out.squeeze(1)\n",
    "                        \n",
    "                output.append(out)\n",
    "                \n",
    "            output = torch.cat(output, dim=0)\n",
    "            \n",
    "        self.net.train(org_training)\n",
    "        return output.numpy()\n",
    "    \n",
    "    def save(self, fn):\n",
    "        ''' Save the model to a file.\n",
    "        \n",
    "        Args:\n",
    "            fn (str): filename.\n",
    "        '''\n",
    "        torch.save(self.net.state_dict(), fn)\n",
    "    \n",
    "    def load(self, fn):\n",
    "        ''' Load the model from a file.\n",
    "        \n",
    "        Args:\n",
    "            fn (str): filename.\n",
    "        '''\n",
    "        state_dict = torch.load(fn, map_location=self.device)\n",
    "        self.net.load_state_dict(state_dict)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TS2Vec(trx_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0: loss=4.249246235318512\n",
      "Epoch #1: loss=2.406607280858615\n",
      "Epoch #2: loss=2.202019851217386\n",
      "Epoch #3: loss=2.044677399913309\n",
      "Epoch #4: loss=1.998812815438398\n",
      "Epoch #5: loss=2.014667148049544\n",
      "Epoch #6: loss=1.904794717607228\n",
      "Epoch #7: loss=1.8790204896617997\n",
      "Epoch #8: loss=1.8386888122751646\n",
      "Epoch #9: loss=1.803795264800068\n",
      "Epoch #10: loss=1.7997330808446474\n",
      "Epoch #11: loss=1.7175399115693715\n",
      "Epoch #12: loss=1.7735411051796515\n",
      "Epoch #13: loss=1.7911778900787416\n",
      "Epoch #14: loss=1.6982846778896656\n",
      "Epoch #15: loss=1.6533252900911246\n",
      "Epoch #16: loss=1.6572428410835112\n",
      "Epoch #17: loss=1.6336104874668815\n",
      "Epoch #18: loss=1.67700676686368\n",
      "Epoch #19: loss=1.7107238972235306\n",
      "Epoch #20: loss=1.6551401248345008\n",
      "Epoch #21: loss=1.6803750366817118\n",
      "Epoch #22: loss=1.6315721992539007\n",
      "Epoch #23: loss=1.549530402851491\n",
      "Epoch #24: loss=1.618836478183144\n",
      "Epoch #25: loss=1.6149939054902267\n",
      "Epoch #26: loss=1.612598803120586\n",
      "Epoch #27: loss=1.6124785559380104\n",
      "Epoch #28: loss=1.6927147999948817\n",
      "Epoch #29: loss=1.635016747814441\n",
      "Epoch #30: loss=1.618752583559708\n",
      "Epoch #31: loss=1.529177398334148\n",
      "Epoch #32: loss=1.5926515028061654\n",
      "Epoch #33: loss=1.5699852548147504\n",
      "Epoch #34: loss=1.5325368843097917\n",
      "Epoch #35: loss=1.531386982574154\n",
      "Epoch #36: loss=1.4648267679851548\n",
      "Epoch #37: loss=1.610720699615324\n",
      "Epoch #38: loss=1.483416305379829\n",
      "Epoch #39: loss=1.5171632042780578\n",
      "Epoch #40: loss=1.516376354433747\n",
      "Epoch #41: loss=1.5805620973409429\n",
      "Epoch #42: loss=1.4937670189842038\n",
      "Epoch #43: loss=1.5951656214138756\n",
      "Epoch #44: loss=1.552167242837821\n",
      "Epoch #45: loss=1.5102267224296384\n",
      "Epoch #46: loss=1.581756353378296\n",
      "Epoch #47: loss=1.5502005936163157\n",
      "Epoch #48: loss=1.590456288594466\n",
      "Epoch #49: loss=1.540316305421142\n",
      "Epoch #50: loss=1.5570553447070874\n",
      "Epoch #51: loss=1.4240310287427322\n"
     ]
    }
   ],
   "source": [
    "loss_log = model.fit(train, 60, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.net.state_dict(), \"averaged_model_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model._net.state_dict(), \"model_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17a5e339af0>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, targets = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seq in dataset:\n",
    "        x = collate_feature_dict([seq]).to(model.device)\n",
    "        emb = model._net.encode(x)\n",
    "        out = model._net(emb)\n",
    "        seq_emb = out.max(dim=1).values.squeeze().cpu().numpy()\n",
    "        embeddings.append(seq_emb)\n",
    "        targets.append(seq[\"global_target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(embeddings)\n",
    "y = np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7586461042982782"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X, y)\n",
    "\n",
    "y_pred = logreg.predict_proba(X)[:, 1]\n",
    "roc_auc_score(y, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf92aa13fedf815d5c8dd192b8d835913fde3e8bc926b2a0ad6cc74ef2ba3ca2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
