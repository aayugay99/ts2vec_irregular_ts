{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\miniconda3\\envs\\ts2vec\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from ptls.preprocessing import PandasDataPreprocessor\n",
    "from ptls.data_load.utils import collate_feature_dict\n",
    "from ptls.data_load.iterable_processing import SeqLenFilter\n",
    "from ptls.data_load.datasets.memory_dataset import MemoryMapDataset\n",
    "\n",
    "from ptls.nn import TrxEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/preprocessed_new/churn.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>mcc_code</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>amount</th>\n",
       "      <th>global_target</th>\n",
       "      <th>holiday_target</th>\n",
       "      <th>weekend_target</th>\n",
       "      <th>churn_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2017-10-21 00:00:00</td>\n",
       "      <td>5023.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-10-12 12:24:07</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2017-12-05 00:00:00</td>\n",
       "      <td>767.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-10-21 00:00:00</td>\n",
       "      <td>2031.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2017-10-24 13:14:24</td>\n",
       "      <td>36562.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490508</th>\n",
       "      <td>10215</td>\n",
       "      <td>37</td>\n",
       "      <td>2016-12-17 00:00:00</td>\n",
       "      <td>2110.9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490509</th>\n",
       "      <td>10215</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-12-16 00:00:00</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490510</th>\n",
       "      <td>10215</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-12-06 00:00:00</td>\n",
       "      <td>182.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490511</th>\n",
       "      <td>10215</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-12-06 13:39:49</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490512</th>\n",
       "      <td>10215</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-12-06 13:42:19</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>490513 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  mcc_code           timestamp   amount  global_target  \\\n",
       "0             0        19 2017-10-21 00:00:00   5023.0              0   \n",
       "1             0         2 2017-10-12 12:24:07  20000.0              0   \n",
       "2             0        10 2017-12-05 00:00:00    767.0              0   \n",
       "3             0         1 2017-10-21 00:00:00   2031.0              0   \n",
       "4             0         9 2017-10-24 13:14:24  36562.0              0   \n",
       "...         ...       ...                 ...      ...            ...   \n",
       "490508    10215        37 2016-12-17 00:00:00   2110.9              0   \n",
       "490509    10215         1 2016-12-16 00:00:00     31.0              0   \n",
       "490510    10215         1 2016-12-06 00:00:00    182.0              0   \n",
       "490511    10215         2 2016-12-06 13:39:49   5000.0              0   \n",
       "490512    10215         2 2016-12-06 13:42:19  30000.0              0   \n",
       "\n",
       "        holiday_target  weekend_target  churn_target  \n",
       "0                    0               1             0  \n",
       "1                    0               0             0  \n",
       "2                    0               0             0  \n",
       "3                    0               1             0  \n",
       "4                    0               0             0  \n",
       "...                ...             ...           ...  \n",
       "490508               0               1             0  \n",
       "490509               0               0             0  \n",
       "490510               0               0             0  \n",
       "490511               0               0             0  \n",
       "490512               0               0             0  \n",
       "\n",
       "[490513 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = PandasDataPreprocessor(\n",
    "    col_id=\"user_id\",\n",
    "    col_event_time=\"timestamp\",\n",
    "    event_time_transformation=\"dt_to_timestamp\",\n",
    "    cols_category=[\"mcc_code\"],\n",
    "    cols_numerical=[\"amount\"],\n",
    "    cols_first_item=\"global_target\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preproc.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MemoryMapDataset(data, [SeqLenFilter(15)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataset, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trx_encoder = TrxEncoder(\n",
    "    embeddings={\n",
    "        \"mcc_code\": {\"in\": 345, \"out\": 24}\n",
    "    },\n",
    "    numeric_values={\n",
    "        \"amount\": \"identity\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "# from models.dilated_conv import DilatedConvEncoder\n",
    "from nn.seq_encoder.dilated_conv import DilatedConvEncoder\n",
    "from ptls.data_load import PaddedBatch\n",
    "\n",
    "def generate_continuous_mask(B, T, n=5, l=0.1):\n",
    "    res = torch.full((B, T), True, dtype=torch.bool)\n",
    "    if isinstance(n, float):\n",
    "        n = int(n * T)\n",
    "    n = max(min(n, T // 2), 1)\n",
    "    \n",
    "    if isinstance(l, float):\n",
    "        l = int(l * T)\n",
    "    l = max(l, 1)\n",
    "    \n",
    "    for i in range(B):\n",
    "        for _ in range(n):\n",
    "            t = np.random.randint(T-l+1)\n",
    "            res[i, t:t+l] = False\n",
    "    return res\n",
    "\n",
    "def generate_binomial_mask(B, T, p=0.5):\n",
    "    return torch.from_numpy(np.random.binomial(1, p, size=(B, T))).to(torch.bool)\n",
    "\n",
    "class TSEncoder(nn.Module):\n",
    "    def __init__(self, trx_encoder, output_dims, depth=10, mask_mode='binomial'):\n",
    "        super().__init__()\n",
    "        self.trx_encoder = trx_encoder\n",
    "        self.output_dims = output_dims\n",
    "        self.hidden_dims = trx_encoder.output_size\n",
    "        self.mask_mode = mask_mode\n",
    "        # self.input_fc = nn.Linear(input_dims, hidden_dims)\n",
    "        self.feature_extractor = DilatedConvEncoder(\n",
    "            self.hidden_dims,\n",
    "            [self.hidden_dims] * depth + [output_dims],\n",
    "            kernel_size=3\n",
    "        )\n",
    "        self.repr_dropout = nn.Dropout(p=0.1)\n",
    "        \n",
    "    def forward(self, x, mask=None):  # x: B x T x input_dims\n",
    "        if mask is None:\n",
    "            if self.training:\n",
    "                mask = self.mask_mode\n",
    "            else:\n",
    "                mask = 'all_true'\n",
    "        \n",
    "        if mask == 'binomial':\n",
    "            mask = generate_binomial_mask(x.size(0), x.size(1)).to(x.device)\n",
    "        elif mask == 'continuous':\n",
    "            mask = generate_continuous_mask(x.size(0), x.size(1)).to(x.device)\n",
    "        elif mask == 'all_true':\n",
    "            mask = x.new_full((x.size(0), x.size(1)), True, dtype=torch.bool)\n",
    "        elif mask == 'all_false':\n",
    "            mask = x.new_full((x.size(0), x.size(1)), False, dtype=torch.bool)\n",
    "        elif mask == 'mask_last':\n",
    "            mask = x.new_full((x.size(0), x.size(1)), True, dtype=torch.bool)\n",
    "            mask[:, -1] = False\n",
    "        \n",
    "        # mask &= nan_mask\n",
    "        x[~mask] = 0\n",
    "        \n",
    "        # conv encoder\n",
    "        x = x.transpose(1, 2)  # B x Ch x T\n",
    "        x = self.repr_dropout(self.feature_extractor(x))  # B x Co x T\n",
    "        x = x.transpose(1, 2)  # B x T x Co\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def encode(self, x: PaddedBatch):\n",
    "        return self.trx_encoder(x).payload\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "from losses.hierarchical_contrastive_loss import hierarchical_contrastive_loss\n",
    "from modules.ts2vec_module import take_per_row\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "class TS2Vec:\n",
    "    '''The TS2Vec model'''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        trx_encoder,\n",
    "        output_dims=320,\n",
    "        depth=10,\n",
    "        device='cuda',\n",
    "        lr=0.001,\n",
    "        batch_size=16,\n",
    "        max_train_length=None,\n",
    "        temporal_unit=0,\n",
    "        after_iter_callback=None,\n",
    "        after_epoch_callback=None\n",
    "    ):\n",
    "        ''' Initialize a TS2Vec model.\n",
    "        \n",
    "        Args:\n",
    "            input_dims (int): The input dimension. For a univariate time series, this should be set to 1.\n",
    "            output_dims (int): The representation dimension.\n",
    "            hidden_dims (int): The hidden dimension of the encoder.\n",
    "            depth (int): The number of hidden residual blocks in the encoder.\n",
    "            device (int): The gpu used for training and inference.\n",
    "            lr (int): The learning rate.\n",
    "            batch_size (int): The batch size.\n",
    "            max_train_length (Union[int, NoneType]): The maximum allowed sequence length for training. For sequence with a length greater than <max_train_length>, it would be cropped into some sequences, each of which has a length less than <max_train_length>.\n",
    "            temporal_unit (int): The minimum unit to perform temporal contrast. When training on a very long sequence, this param helps to reduce the cost of time and memory.\n",
    "            after_iter_callback (Union[Callable, NoneType]): A callback function that would be called after each iteration.\n",
    "            after_epoch_callback (Union[Callable, NoneType]): A callback function that would be called after each epoch.\n",
    "        '''\n",
    "        \n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.max_train_length = max_train_length\n",
    "        self.temporal_unit = temporal_unit\n",
    "        \n",
    "        self._net = TSEncoder(trx_encoder=trx_encoder, output_dims=output_dims, depth=depth).to(self.device)\n",
    "        self.net = torch.optim.swa_utils.AveragedModel(self._net)\n",
    "        self.net.update_parameters(self._net)\n",
    "        \n",
    "        self.after_iter_callback = after_iter_callback\n",
    "        self.after_epoch_callback = after_epoch_callback\n",
    "        \n",
    "        self.n_epochs = 0\n",
    "        self.n_iters = 0\n",
    "    \n",
    "    def fit(self, train_dataset, n_epochs=None, n_iters=None, verbose=False):\n",
    "        ''' Training the TS2Vec model.\n",
    "        \n",
    "        Args:\n",
    "            train_data (numpy.ndarray): The training data. It should have a shape of (n_instance, n_timestamps, n_features). All missing data should be set to NaN.\n",
    "            n_epochs (Union[int, NoneType]): The number of epochs. When this reaches, the training stops.\n",
    "            n_iters (Union[int, NoneType]): The number of iterations. When this reaches, the training stops. If both n_epochs and n_iters are not specified, a default setting would be used that sets n_iters to 200 for a dataset with size <= 100000, 600 otherwise.\n",
    "            verbose (bool): Whether to print the training loss after each epoch.\n",
    "            \n",
    "        Returns:\n",
    "            loss_log: a list containing the training losses on each epoch.\n",
    "        '''\n",
    "        # assert train_data.ndim == 3\n",
    "        \n",
    "        # if n_iters is None and n_epochs is None:\n",
    "        #     n_iters = 200 if train_data.size <= 100000 else 600  # default param for n_iters\n",
    "        \n",
    "        # if self.max_train_length is not None:\n",
    "        #     sections = train_data.shape[1] // self.max_train_length\n",
    "        #     if sections >= 2:\n",
    "        #         train_data = np.concatenate(split_with_nan(train_data, sections, axis=1), axis=0)\n",
    "\n",
    "        # temporal_missing = np.isnan(train_data).all(axis=-1).any(axis=0)\n",
    "        # if temporal_missing[0] or temporal_missing[-1]:\n",
    "        #     train_data = centerize_vary_length_series(train_data)\n",
    "                \n",
    "        # train_data = train_data[~np.isnan(train_data).all(axis=2).all(axis=1)]\n",
    "        \n",
    "        # train_dataset = TensorDataset(torch.from_numpy(train_data).to(torch.float))\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=min(self.batch_size, len(train_dataset)), \n",
    "            shuffle=True, \n",
    "            drop_last=True,\n",
    "            collate_fn=collate_feature_dict\n",
    "        )\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(self._net.parameters(), lr=self.lr)\n",
    "        \n",
    "        loss_log = []\n",
    "        \n",
    "        while True:\n",
    "            if n_epochs is not None and self.n_epochs >= n_epochs:\n",
    "                break\n",
    "            \n",
    "            cum_loss = 0\n",
    "            n_epoch_iters = 0\n",
    "            \n",
    "            interrupted = False\n",
    "            for batch in train_loader:\n",
    "                if n_iters is not None and self.n_iters >= n_iters:\n",
    "                    interrupted = True\n",
    "                    break\n",
    "                \n",
    "                x = self._net.encode(batch.to(self.device))\n",
    "                if self.max_train_length is not None and x.size(1) > self.max_train_length:\n",
    "                    window_offset = np.random.randint(x.size(1) - self.max_train_length + 1)\n",
    "                    x = x[:, window_offset : window_offset + self.max_train_length]\n",
    "                # x = x.to(self.device)\n",
    "                \n",
    "                ts_l = x.size(1)\n",
    "                crop_l = np.random.randint(low=2 ** (self.temporal_unit + 1), high=ts_l+1)\n",
    "                crop_left = np.random.randint(ts_l - crop_l + 1)\n",
    "                crop_right = crop_left + crop_l\n",
    "                crop_eleft = np.random.randint(crop_left + 1)\n",
    "                crop_eright = np.random.randint(low=crop_right, high=ts_l + 1)\n",
    "                crop_offset = np.random.randint(low=-crop_eleft, high=ts_l - crop_eright + 1, size=x.size(0))\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                out1 = self._net(take_per_row(x, crop_offset + crop_eleft, crop_right - crop_eleft))\n",
    "                out1 = out1[:, -crop_l:]\n",
    "                \n",
    "                out2 = self._net(take_per_row(x, crop_offset + crop_left, crop_eright - crop_left))\n",
    "                out2 = out2[:, :crop_l]\n",
    "                \n",
    "                loss = hierarchical_contrastive_loss(\n",
    "                    out1,\n",
    "                    out2,\n",
    "                    temporal_unit=self.temporal_unit\n",
    "                )\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                self.net.update_parameters(self._net)\n",
    "                    \n",
    "                cum_loss += loss.item()\n",
    "                n_epoch_iters += 1\n",
    "                \n",
    "                self.n_iters += 1\n",
    "                \n",
    "                if self.after_iter_callback is not None:\n",
    "                    self.after_iter_callback(self, loss.item())\n",
    "            \n",
    "            if interrupted:\n",
    "                break\n",
    "            \n",
    "            cum_loss /= n_epoch_iters\n",
    "            loss_log.append(cum_loss)\n",
    "            if verbose:\n",
    "                print(f\"Epoch #{self.n_epochs}: loss={cum_loss}\")\n",
    "            self.n_epochs += 1\n",
    "            \n",
    "            if self.after_epoch_callback is not None:\n",
    "                self.after_epoch_callback(self, cum_loss)\n",
    "            \n",
    "        return loss_log\n",
    "    \n",
    "    def _eval_with_pooling(self, x, mask=None, slicing=None, encoding_window=None):\n",
    "        out = self.net(x.to(self.device, non_blocking=True), mask)\n",
    "        if encoding_window == 'full_series':\n",
    "            if slicing is not None:\n",
    "                out = out[:, slicing]\n",
    "            out = F.max_pool1d(\n",
    "                out.transpose(1, 2),\n",
    "                kernel_size = out.size(1),\n",
    "            ).transpose(1, 2)\n",
    "            \n",
    "        elif isinstance(encoding_window, int):\n",
    "            out = F.max_pool1d(\n",
    "                out.transpose(1, 2),\n",
    "                kernel_size = encoding_window,\n",
    "                stride = 1,\n",
    "                padding = encoding_window // 2\n",
    "            ).transpose(1, 2)\n",
    "            if encoding_window % 2 == 0:\n",
    "                out = out[:, :-1]\n",
    "            if slicing is not None:\n",
    "                out = out[:, slicing]\n",
    "            \n",
    "        elif encoding_window == 'multiscale':\n",
    "            p = 0\n",
    "            reprs = []\n",
    "            while (1 << p) + 1 < out.size(1):\n",
    "                t_out = F.max_pool1d(\n",
    "                    out.transpose(1, 2),\n",
    "                    kernel_size = (1 << (p + 1)) + 1,\n",
    "                    stride = 1,\n",
    "                    padding = 1 << p\n",
    "                ).transpose(1, 2)\n",
    "                if slicing is not None:\n",
    "                    t_out = t_out[:, slicing]\n",
    "                reprs.append(t_out)\n",
    "                p += 1\n",
    "            out = torch.cat(reprs, dim=-1)\n",
    "            \n",
    "        else:\n",
    "            if slicing is not None:\n",
    "                out = out[:, slicing]\n",
    "            \n",
    "        return out.cpu()\n",
    "    \n",
    "    def encode(self, data, mask=None, encoding_window=None, causal=False, sliding_length=None, sliding_padding=0, batch_size=None):\n",
    "        ''' Compute representations using the model.\n",
    "        \n",
    "        Args:\n",
    "            data (numpy.ndarray): This should have a shape of (n_instance, n_timestamps, n_features). All missing data should be set to NaN.\n",
    "            mask (str): The mask used by encoder can be specified with this parameter. This can be set to 'binomial', 'continuous', 'all_true', 'all_false' or 'mask_last'.\n",
    "            encoding_window (Union[str, int]): When this param is specified, the computed representation would the max pooling over this window. This can be set to 'full_series', 'multiscale' or an integer specifying the pooling kernel size.\n",
    "            causal (bool): When this param is set to True, the future informations would not be encoded into representation of each timestamp.\n",
    "            sliding_length (Union[int, NoneType]): The length of sliding window. When this param is specified, a sliding inference would be applied on the time series.\n",
    "            sliding_padding (int): This param specifies the contextual data length used for inference every sliding windows.\n",
    "            batch_size (Union[int, NoneType]): The batch size used for inference. If not specified, this would be the same batch size as training.\n",
    "            \n",
    "        Returns:\n",
    "            repr: The representations for data.\n",
    "        '''\n",
    "        assert self.net is not None, 'please train or load a net first'\n",
    "        assert data.ndim == 3\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "        n_samples, ts_l, _ = data.shape\n",
    "\n",
    "        org_training = self.net.training\n",
    "        self.net.eval()\n",
    "        \n",
    "        dataset = TensorDataset(torch.from_numpy(data).to(torch.float))\n",
    "        loader = DataLoader(dataset, batch_size=batch_size)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = []\n",
    "            for batch in loader:\n",
    "                x = batch[0]\n",
    "                if sliding_length is not None:\n",
    "                    reprs = []\n",
    "                    if n_samples < batch_size:\n",
    "                        calc_buffer = []\n",
    "                        calc_buffer_l = 0\n",
    "                    for i in range(0, ts_l, sliding_length):\n",
    "                        l = i - sliding_padding\n",
    "                        r = i + sliding_length + (sliding_padding if not causal else 0)\n",
    "                        x_sliding = torch_pad_nan(\n",
    "                            x[:, max(l, 0) : min(r, ts_l)],\n",
    "                            left=-l if l<0 else 0,\n",
    "                            right=r-ts_l if r>ts_l else 0,\n",
    "                            dim=1\n",
    "                        )\n",
    "                        if n_samples < batch_size:\n",
    "                            if calc_buffer_l + n_samples > batch_size:\n",
    "                                out = self._eval_with_pooling(\n",
    "                                    torch.cat(calc_buffer, dim=0),\n",
    "                                    mask,\n",
    "                                    slicing=slice(sliding_padding, sliding_padding+sliding_length),\n",
    "                                    encoding_window=encoding_window\n",
    "                                )\n",
    "                                reprs += torch.split(out, n_samples)\n",
    "                                calc_buffer = []\n",
    "                                calc_buffer_l = 0\n",
    "                            calc_buffer.append(x_sliding)\n",
    "                            calc_buffer_l += n_samples\n",
    "                        else:\n",
    "                            out = self._eval_with_pooling(\n",
    "                                x_sliding,\n",
    "                                mask,\n",
    "                                slicing=slice(sliding_padding, sliding_padding+sliding_length),\n",
    "                                encoding_window=encoding_window\n",
    "                            )\n",
    "                            reprs.append(out)\n",
    "\n",
    "                    if n_samples < batch_size:\n",
    "                        if calc_buffer_l > 0:\n",
    "                            out = self._eval_with_pooling(\n",
    "                                torch.cat(calc_buffer, dim=0),\n",
    "                                mask,\n",
    "                                slicing=slice(sliding_padding, sliding_padding+sliding_length),\n",
    "                                encoding_window=encoding_window\n",
    "                            )\n",
    "                            reprs += torch.split(out, n_samples)\n",
    "                            calc_buffer = []\n",
    "                            calc_buffer_l = 0\n",
    "                    \n",
    "                    out = torch.cat(reprs, dim=1)\n",
    "                    if encoding_window == 'full_series':\n",
    "                        out = F.max_pool1d(\n",
    "                            out.transpose(1, 2).contiguous(),\n",
    "                            kernel_size = out.size(1),\n",
    "                        ).squeeze(1)\n",
    "                else:\n",
    "                    out = self._eval_with_pooling(x, mask, encoding_window=encoding_window)\n",
    "                    if encoding_window == 'full_series':\n",
    "                        out = out.squeeze(1)\n",
    "                        \n",
    "                output.append(out)\n",
    "                \n",
    "            output = torch.cat(output, dim=0)\n",
    "            \n",
    "        self.net.train(org_training)\n",
    "        return output.numpy()\n",
    "    \n",
    "    def save(self, fn):\n",
    "        ''' Save the model to a file.\n",
    "        \n",
    "        Args:\n",
    "            fn (str): filename.\n",
    "        '''\n",
    "        torch.save(self.net.state_dict(), fn)\n",
    "    \n",
    "    def load(self, fn):\n",
    "        ''' Load the model from a file.\n",
    "        \n",
    "        Args:\n",
    "            fn (str): filename.\n",
    "        '''\n",
    "        state_dict = torch.load(fn, map_location=self.device)\n",
    "        self.net.load_state_dict(state_dict)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TS2Vec(trx_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #0: loss=4.10163254486887\n",
      "Epoch #1: loss=2.4674146151253087\n",
      "Epoch #2: loss=2.2208538668358373\n",
      "Epoch #3: loss=2.143957720111739\n",
      "Epoch #4: loss=2.0194489704935172\n",
      "Epoch #5: loss=1.9804518922620458\n",
      "Epoch #6: loss=1.8940160482035957\n",
      "Epoch #7: loss=1.880510822481472\n",
      "Epoch #8: loss=1.8383355396479248\n",
      "Epoch #9: loss=1.8440039529491532\n",
      "Epoch #10: loss=1.7273891592315334\n",
      "Epoch #11: loss=1.7942965238200508\n",
      "Epoch #12: loss=1.7839685007628159\n",
      "Epoch #13: loss=1.720567142432518\n",
      "Epoch #14: loss=1.7506577196391488\n",
      "Epoch #15: loss=1.6533387170629463\n",
      "Epoch #16: loss=1.702924209326385\n",
      "Epoch #17: loss=1.6740076027901067\n",
      "Epoch #18: loss=1.6741861802363684\n",
      "Epoch #19: loss=1.5927217286607998\n",
      "Epoch #20: loss=1.64022248862726\n",
      "Epoch #21: loss=1.6367495865956974\n",
      "Epoch #22: loss=1.5495441222963062\n",
      "Epoch #23: loss=1.616086774268131\n",
      "Epoch #24: loss=1.6205256711616207\n",
      "Epoch #25: loss=1.5502154856075643\n",
      "Epoch #26: loss=1.686340765431825\n",
      "Epoch #27: loss=1.6794858681045564\n",
      "Epoch #28: loss=1.646144531516411\n",
      "Epoch #29: loss=1.5511448870786289\n",
      "Epoch #30: loss=1.5909236032470517\n",
      "Epoch #31: loss=1.5868178825629384\n",
      "Epoch #32: loss=1.6135178189045987\n",
      "Epoch #33: loss=1.5940179938246846\n",
      "Epoch #34: loss=1.5713395688697878\n",
      "Epoch #35: loss=1.507890984114365\n",
      "Epoch #36: loss=1.570273555724727\n",
      "Epoch #37: loss=1.5039403371000097\n",
      "Epoch #38: loss=1.5431562073317617\n",
      "Epoch #39: loss=1.602450591108577\n",
      "Epoch #40: loss=1.4982928230694914\n",
      "Epoch #41: loss=1.5505111982465274\n",
      "Epoch #42: loss=1.5548470278500546\n",
      "Epoch #43: loss=1.4810244243154642\n",
      "Epoch #44: loss=1.4909448905995017\n",
      "Epoch #45: loss=1.5012476195690603\n",
      "Epoch #46: loss=1.520972884618319\n",
      "Epoch #47: loss=1.5492107969546607\n",
      "Epoch #48: loss=1.4828990147663996\n",
      "Epoch #49: loss=1.5449734707834266\n"
     ]
    }
   ],
   "source": [
    "loss_log = model.fit(train, 50, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.net.state_dict(), \"averaged_model_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model._net.state_dict(), \"model_dict.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model._net.load_state_dict(torch.load(\"model_dict.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b1e082be50>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_data(data, model):\n",
    "    embeddings, targets = [], []\n",
    "    model._net.eval()\n",
    "    with torch.no_grad():\n",
    "        for seq in data:\n",
    "            x = collate_feature_dict([seq]).to(model.device)\n",
    "            emb = model._net.encode(x)\n",
    "            out = model._net(emb)\n",
    "            seq_emb = out.max(dim=1).values.squeeze().cpu().numpy()\n",
    "            embeddings.append(seq_emb)\n",
    "            targets.append(seq[\"global_target\"])\n",
    "\n",
    "    return np.array(embeddings), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = embed_data(train, model)\n",
    "X_test, y_test = embed_data(test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6713143056951789"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# lgbm = LGBMClassifier(n_estimators=500, verbose=-1)\n",
    "# lgbm.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = lgbm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTLS models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TS2VecDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        feature_arrays = self.data[item]\n",
    "        return feature_arrays\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        return collate_feature_dict(batch), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def hierarchical_contrastive_loss(z1, z2, alpha=0.5, temporal_unit=0):\n",
    "    loss = torch.tensor(0., device=z1.device)\n",
    "    d = 0\n",
    "    while z1.size(1) > 1:\n",
    "        if alpha != 0:\n",
    "            loss += alpha * instance_contrastive_loss(z1, z2)\n",
    "        if d >= temporal_unit:\n",
    "            if 1 - alpha != 0:\n",
    "                loss += (1 - alpha) * temporal_contrastive_loss(z1, z2)\n",
    "        d += 1\n",
    "        z1 = F.max_pool1d(z1.transpose(1, 2), kernel_size=2).transpose(1, 2)\n",
    "        z2 = F.max_pool1d(z2.transpose(1, 2), kernel_size=2).transpose(1, 2)\n",
    "    if z1.size(1) == 1:\n",
    "        if alpha != 0:\n",
    "            loss += alpha * instance_contrastive_loss(z1, z2)\n",
    "        d += 1\n",
    "    return loss / d\n",
    "\n",
    "\n",
    "def instance_contrastive_loss(z1, z2):\n",
    "    B, T = z1.size(0), z1.size(1)\n",
    "    if B == 1:\n",
    "        return z1.new_tensor(0.)\n",
    "    z = torch.cat([z1, z2], dim=0)  # 2B x T x C\n",
    "    z = z.transpose(0, 1)  # T x 2B x C\n",
    "    sim = torch.matmul(z, z.transpose(1, 2))  # T x 2B x 2B\n",
    "    logits = torch.tril(sim, diagonal=-1)[:, :, :-1]    # T x 2B x (2B-1)\n",
    "    logits += torch.triu(sim, diagonal=1)[:, :, 1:]\n",
    "    logits = -F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    i = torch.arange(B, device=z1.device)\n",
    "    loss = (logits[:, i, B + i - 1].mean() + logits[:, B + i, i].mean()) / 2\n",
    "    return loss\n",
    "\n",
    "\n",
    "def temporal_contrastive_loss(z1, z2):\n",
    "    B, T = z1.size(0), z1.size(1)\n",
    "    if T == 1:\n",
    "        return z1.new_tensor(0.)\n",
    "    z = torch.cat([z1, z2], dim=1)  # B x 2T x C\n",
    "    sim = torch.matmul(z, z.transpose(1, 2))  # B x 2T x 2T\n",
    "    logits = torch.tril(sim, diagonal=-1)[:, :, :-1]    # B x 2T x (2T-1)\n",
    "    logits += torch.triu(sim, diagonal=1)[:, :, 1:]\n",
    "    logits = -F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    t = torch.arange(T, device=z1.device)\n",
    "    loss = (logits[:, t, T + t - 1].mean() + logits[:, T + t, t].mean()) / 2\n",
    "    return loss\n",
    "\n",
    "\n",
    "def take_per_row(A, indx, num_elem):\n",
    "    all_indx = indx[:,None] + np.arange(num_elem)\n",
    "    return A[torch.arange(all_indx.shape[0])[:,None], all_indx]\n",
    "\n",
    "\n",
    "class HierarchicalContrastiveLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, temporal_unit=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.temporal_unit = temporal_unit\n",
    "\n",
    "    def forward(self, embeddings, _):\n",
    "        out1, out2 = embeddings\n",
    "        return hierarchical_contrastive_loss(out1, out2, self.alpha, self.temporal_unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.frames.abs_module import ABSModule\n",
    "from torchmetrics import MeanMetric\n",
    "\n",
    "\n",
    "class TS2Vec(ABSModule):\n",
    "    '''The TS2Vec model'''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_encoder,\n",
    "        loss=None,\n",
    "        validation_metric=None,\n",
    "        optimizer_partial=None,\n",
    "        lr_scheduler_partial=None\n",
    "    ):\n",
    "        ''' Initialize a TS2Vec model.\n",
    "        \n",
    "        Args:\n",
    "        '''\n",
    "        \n",
    "        if loss is None:\n",
    "            loss = HierarchicalContrastiveLoss(alpha=0.5, temporal_unit=0)\n",
    "\n",
    "        self.temporal_unit = loss.temporal_unit\n",
    "        \n",
    "        # if validation_metric is None:\n",
    "\n",
    "        super().__init__(validation_metric,\n",
    "                         seq_encoder,\n",
    "                         loss,\n",
    "                         optimizer_partial,\n",
    "                         lr_scheduler_partial)\n",
    "\n",
    "        self.valid_loss = MeanMetric()\n",
    "\n",
    "    def shared_step(self, x, y):\n",
    "        trx_encoder = self._seq_encoder.trx_encoder\n",
    "        seq_encoder = self._seq_encoder.seq_encoder \n",
    "\n",
    "        seq_lens = x.seq_lens\n",
    "        x = trx_encoder(x).payload\n",
    "\n",
    "        ts_l = x.size(1)\n",
    "        crop_l = np.random.randint(low=2 ** (self.temporal_unit + 1), high=ts_l+1)\n",
    "        crop_left = np.random.randint(ts_l - crop_l + 1)\n",
    "        crop_right = crop_left + crop_l\n",
    "        crop_eleft = np.random.randint(crop_left + 1)\n",
    "        crop_eright = np.random.randint(low=crop_right, high=ts_l + 1)\n",
    "        crop_offset = np.random.randint(low=-crop_eleft, high=ts_l - crop_eright + 1, size=x.size(0))\n",
    "\n",
    "        input1 = PaddedBatch(take_per_row(x, crop_offset + crop_eleft, crop_right - crop_eleft), seq_lens)\n",
    "        input2 = PaddedBatch(take_per_row(x, crop_offset + crop_left, crop_eright - crop_left), seq_lens)\n",
    "        \n",
    "        out1 = seq_encoder(input1).payload\n",
    "        out1 = out1[:, -crop_l:]\n",
    "                \n",
    "        out2 = seq_encoder(input2).payload\n",
    "        out2 = out2[:, :crop_l]\n",
    "        \n",
    "        return (out1, out2), y\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        y_h, y = self.shared_step(*batch)\n",
    "        loss = self._loss(y_h, y)\n",
    "        self.valid_loss(loss)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        self.log(f'valid_loss', self.valid_loss, prog_bar=True)\n",
    "        # self._validation_metric.reset()\n",
    "\n",
    "    @property\n",
    "    def is_requires_reduced_sequence(self):\n",
    "        return False\n",
    "    \n",
    "    @property\n",
    "    def metric_name(self):\n",
    "        return \"valid_loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.nn.seq_encoder.containers import SeqEncoderContainer\n",
    "from ptls.nn.seq_encoder.abs_seq_encoder import AbsSeqEncoder\n",
    "from ptls.data_load.padded_batch import PaddedBatch\n",
    "from nn.seq_encoder.dilated_conv import DilatedConvEncoder\n",
    "\n",
    "\n",
    "def generate_binomial_mask(B, T, p=0.5):\n",
    "    return torch.from_numpy(np.random.binomial(1, p, size=(B, T))).to(torch.bool)\n",
    "\n",
    "\n",
    "class ConvEncoder(AbsSeqEncoder):\n",
    "    def __init__(self,\n",
    "                 input_size=None,\n",
    "                 hidden_size=None,\n",
    "                 num_layers=10,\n",
    "                 dropout=0,\n",
    "                 mask_mode='binomial',\n",
    "                 is_reduce_sequence=False,  \n",
    "                 reducer='maxpool'\n",
    "                 ):\n",
    "        \n",
    "        super().__init__(is_reduce_sequence=is_reduce_sequence)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.mask_mode = mask_mode\n",
    "\n",
    "        self.feature_extractor = DilatedConvEncoder(\n",
    "            input_size,\n",
    "            [input_size] * num_layers + [hidden_size],\n",
    "            kernel_size=3\n",
    "        )\n",
    "\n",
    "        self.repr_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.reducer = reducer\n",
    "\n",
    "    def forward(self, x: PaddedBatch, mask=None):  # x: B x T x input_dims\n",
    "        shape = x.payload.size()\n",
    "\n",
    "        # generate & apply mask\n",
    "        if mask is None:\n",
    "            if self.training:\n",
    "                mask = self.mask_mode\n",
    "            else:\n",
    "                mask = 'all_true'\n",
    "        \n",
    "        if mask == 'binomial':\n",
    "            mask = generate_binomial_mask(shape[0], shape[1]).to(x.device)\n",
    "        elif mask == 'continuous':\n",
    "            mask = generate_continuous_mask(shape[0], shape[1]).to(x.device)\n",
    "        elif mask == 'all_true':\n",
    "            mask = x.payload.new_full((shape[0], shape[1]), True, dtype=torch.bool)\n",
    "        elif mask == 'all_false':\n",
    "            mask = x.payload.new_full((shape[0], shape[1]), False, dtype=torch.bool)\n",
    "        elif mask == 'mask_last':\n",
    "            mask = x.payload.new_full((shape[0], shape[1]), True, dtype=torch.bool)\n",
    "            mask[:, -1] = False\n",
    "        \n",
    "        x_masked = x.payload\n",
    "        x_masked[~mask] = 0\n",
    "        \n",
    "        # conv encoder\n",
    "        x_masked = x_masked.transpose(1, 2)  # B x Ch x T\n",
    "        out = self.repr_dropout(self.feature_extractor(x_masked))  # B x Co x T\n",
    "        out = out.transpose(1, 2)  # B x T x Co\n",
    "\n",
    "        out = PaddedBatch(out, x.seq_lens)\n",
    "        if self.is_reduce_sequence:\n",
    "            out = out.payload.max(dim=1).values\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvSeqEncoder(SeqEncoderContainer):\n",
    "    def __init__(self,\n",
    "                trx_encoder=None,\n",
    "                input_size=None,\n",
    "                is_reduce_sequence=False,\n",
    "                **seq_encoder_params,\n",
    "                ):\n",
    "        super().__init__(\n",
    "            trx_encoder=trx_encoder,\n",
    "            seq_encoder_cls=ConvEncoder,\n",
    "            input_size=input_size,\n",
    "            seq_encoder_params=seq_encoder_params,\n",
    "            is_reduce_sequence=is_reduce_sequence,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptls.frames import PtlsDataModule\n",
    "\n",
    "datamodule: PtlsDataModule = PtlsDataModule(\n",
    "    train_data=TS2VecDataset(train),\n",
    "    valid_data=TS2VecDataset(test),\n",
    "    train_batch_size=16,\n",
    "    valid_batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_encoder = ConvSeqEncoder(\n",
    "    TrxEncoder(\n",
    "        embeddings={\"mcc_code\": {\"in\": 345, \"out\": 24}},\n",
    "        numeric_values={\"amount\": \"identity\"}\n",
    "    ),  \n",
    "    **{\n",
    "        \"hidden_size\": 320,\n",
    "        \"num_layers\": 10,\n",
    "        \"dropout\": 0.1,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.optim import AdamW\n",
    "from ptls.frames.coles.coles_module import CoLESModule \n",
    "\n",
    "scheduler_partial = partial(ReduceLROnPlateau, factor=.9025, patience=50)\n",
    "optimizer_partial = partial(AdamW, lr=0.001, weight_decay=0.0)\n",
    "\n",
    "model = TS2Vec(seq_encoder, optimizer_partial=optimizer_partial, lr_scheduler_partial=scheduler_partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = collate_feature_dict([train[i] for i in range(10)])\n",
    "\n",
    "# model(batch).payload.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name         | Type                        | Params\n",
      "-------------------------------------------------------------\n",
      "0 | _loss        | HierarchicalContrastiveLoss | 0     \n",
      "1 | _seq_encoder | ConvSeqEncoder              | 386 K \n",
      "2 | valid_loss   | MeanMetric                  | 0     \n",
      "-------------------------------------------------------------\n",
      "386 K     Trainable params\n",
      "0         Non-trainable params\n",
      "386 K     Total params\n",
      "1.546     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4bbb87c4e64b46acd9777c6e1576e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc202665cd2a4c9589a201e5192db894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_data(data, model):\n",
    "    embeddings, targets = [], []\n",
    "\n",
    "    model.eval()\n",
    "    model.training = False\n",
    "    model.seq_encoder.is_reduce_sequence = True \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for seq in data:\n",
    "            x = collate_feature_dict([seq]).to(model.device)\n",
    "            seq_emb = model(x).squeeze().cpu().numpy()\n",
    "            embeddings.append(seq_emb)\n",
    "            targets.append(seq[\"global_target\"])\n",
    "\n",
    "    return np.array(embeddings), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = embed_data(train, model)\n",
    "X_test, y_test = embed_data(test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3963, 320)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6781764937278728"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# lgbm = LGBMClassifier(n_estimators=1000, verbose=-1, max_depth=2, reg_alpha=1, reg_lambda=1)\n",
    "# lgbm.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = lgbm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "roc_auc_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e48c7b9d2bc3575b1270a190a30356d43d076e9d316773e23ee2f599d00b1f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
